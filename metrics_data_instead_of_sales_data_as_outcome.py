# -*- coding: utf-8 -*-
"""metrics data instead of sales data as outcome.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VFqm9y_yokJjqkE3buNt3hECPiVKqSjU
"""

!pip install --upgrade pip
!pip install --upgrade git+https://github.com/google/lightweight_mmm.git

import pandas as pd
from lightweight_mmm import preprocessing, lightweight_mmm, plot, optimize_media
import jax.numpy as jnp
from sklearn.metrics import mean_absolute_percentage_error
import jax.numpy as jnp
import numpyro
from lightweight_mmm import lightweight_mmm
from lightweight_mmm import optimize_media
from lightweight_mmm import plot
from lightweight_mmm import preprocessing
from lightweight_mmm import utils

ins_data = pd.read_csv("/content/instagram_processed_data.csv", usecols=["Impressions", "Reach", "Engaged Users", "Fan Growth", "Views", "Cost", "Consideration", "Awareness", "Engagement"])

yt_data = pd.read_csv("/content/youtube_processed_data.csv", usecols=["Impressions", "Reach", "Engaged Users", "Fan Growth", "Views", "Cost", "Consideration", "Awareness", "Engagement"])

fb_data = pd.read_csv("/content/facebook_processed_data.csv", usecols=["Lifetime Post Total Impressions", "Reach", "Engaged Users", "Fan Growth", "Views", "Cost", "Consideration", "Awareness", "Engagement"])

ins_data.shape

fb_data.shape

yt_data.shape

ins_data = ins_data[:398]

ins_data.shape

yt_data = yt_data[:398]

yt_data.shape

type(fb_data)

ins_media_data = ins_data[["Impressions"]]

fb_media_data = fb_data[["Lifetime Post Total Impressions"]]

yt_media_data = yt_data[["Impressions"]]

ins_cost_data = ins_data[["Cost"]]

fb_cost_data = fb_data[["Cost"]]

yt_cost_data = yt_data[["Cost"]]

ins_sales_data = ins_data[["Awareness"]]

fb_sales_data = fb_data[["Awareness"]]

yt_sales_data = yt_data[["Awareness"]]

ins_extra_features_data = ins_data[["Reach", "Engaged Users", "Fan Growth", "Views"]]

fb_extra_features_data = fb_data[["Reach", "Engaged Users", "Fan Growth", "Views"]]

yt_extra_features_data = yt_data[["Reach", "Engaged Users", "Fan Growth", "Views"]]

# Reset index if the dataframes do not align properly
ins_media_data.reset_index(drop=True, inplace=True)
fb_media_data.reset_index(drop=True, inplace=True)
yt_media_data.reset_index(drop=True, inplace=True)

# Concatenate and rename columns
across_platform_media_data_df = pd.concat([fb_media_data, ins_media_data, yt_media_data], axis=1)
across_platform_media_data_df.columns = ['fb_media_data', 'ins_media_data', 'yt_media_data']

across_platform_media_data = across_platform_media_data_df.to_numpy()

across_platform_media_data

# Reset index if the dataframes do not align properly
ins_cost_data = ins_cost_data.sum(axis=0)
fb_cost_data = fb_cost_data.sum(axis=0)
yt_cost_data = yt_cost_data.sum(axis=0)

# Concatenate and rename columns
#across_platform_cost_data_df = pd.concat([fb_cost_data, ins_cost_data, yt_cost_data], axis=1)
#across_platform_cost_data_df.columns = ['fb_cost_data', 'ins_cost_data', 'yt_cost_data']

type(ins_cost_data.to_numpy().reshape(-1, 1))

import numpy as np
across_platform_cost_data = np.concatenate([fb_cost_data, ins_cost_data, yt_cost_data], axis=0)

across_platform_cost_data

# Reset index if the dataframes do not align properly
ins_sales_data.reset_index(drop=True, inplace=True)
fb_sales_data.reset_index(drop=True, inplace=True)
yt_sales_data.reset_index(drop=True, inplace=True)

# Concatenate and rename columns
# across_platform_sales_data_df = pd.concat([fb_sales_data, ins_sales_data, yt_sales_data], axis=1)
across_platform_sales_data_df = (fb_sales_data + ins_sales_data + yt_sales_data)/3
across_platform_sales_data_df.columns = ['across_platform_sales_data']



across_platform_sales_data = across_platform_sales_data_df.to_numpy()

across_platform_sales_data.shape

# Reset index if the dataframes do not align properly
ins_sales_data.reset_index(drop=True, inplace=True)
fb_sales_data.reset_index(drop=True, inplace=True)
yt_sales_data.reset_index(drop=True, inplace=True)

# Concatenate and rename columns
# across_platform_sales_data_df = pd.concat([fb_sales_data, ins_sales_data, yt_sales_data], axis=1)
across_platform_sales_data_df1 = (fb_sales_data + ins_sales_data + yt_sales_data)
across_platform_sales_data_df1.columns = ['across_platform_sales_data']

across_platform_sales_data1 = across_platform_sales_data_df1.to_numpy()

across_platform_sales_data1.shape

# Reset index if the dataframes do not align properly
ins_extra_features_data.reset_index(drop=True, inplace=True)
fb_extra_features_data.reset_index(drop=True, inplace=True)
yt_extra_features_data.reset_index(drop=True, inplace=True)

# Concatenate and rename columns
across_platform_extra_features_data_df = pd.concat([fb_extra_features_data, ins_extra_features_data, yt_extra_features_data], axis=1)
across_platform_extra_features_data_df.columns = ['fb_reach_data', 'fb_engaged_user_data', 'fb_fan_growth_data', 'fb_views_data', 'ins_reach_data', 'ins_engaged_user_data', 'ins_fan_growth_data', 'ins_views_data', 'yt_reach_data', 'yt_engaged_user_data', 'yt_fan_growth_data', 'yt_views_data']

across_platform_extra_features_data = across_platform_extra_features_data_df.to_numpy()
across_platform_extra_features_data.shape

media_data_train = across_platform_media_data[:300]
media_data_test = across_platform_media_data[300:]
target_data_train = across_platform_sales_data[:300]
target_data_test = across_platform_sales_data[300:]
target_data_train1 = across_platform_sales_data1[:300]
target_data_test1 = across_platform_sales_data1[300:]
cost_data_train = across_platform_cost_data
#cost_data_train = cost_data_train.reshape(-1, 1)
#cost_data_test = cost_data[388:].sum(axis=0)
#cost_data_test = cost_data_test.reshape(-1, 1)
extra_features_data_train = across_platform_extra_features_data[:300]
extra_features_data_test = across_platform_extra_features_data[300:]

media_data_train

media_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)
target_scaler = preprocessing.CustomScaler(
    divide_operation=jnp.mean)
cost_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)
extra_features_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)

media_data_train_scaled = media_scaler.fit_transform(media_data_train)
target_train_scaled = target_scaler.fit_transform(target_data_train)
target_train_scaled1 = target_scaler.fit_transform(target_data_train1)
costs_scaled = cost_scaler.fit_transform(cost_data_train)
extra_features_scaled = extra_features_scaler.fit_transform(extra_features_data_train)

media_data_test_scaled = media_scaler.transform(media_data_test)
extra_features_test_scaled = extra_features_scaler.fit_transform(extra_features_data_test)

target_train_scaled.shape

costs_scaled.shape

media_data_train_scaled.shape

mmm = lightweight_mmm.LightweightMMM(model_name="carryover")

# For replicability in terms of random number generation in sampling
# reuse the same seed for different trainings.
mmm.fit(
    media=media_data_train_scaled,
    media_prior=costs_scaled,
    target=target_train_scaled,
    extra_features=extra_features_scaled,
    number_warmup=1000,
    number_samples=1000,
    seed=1)

plot.plot_response_curves(
    media_mix_model=mmm, target_scaler=target_scaler, seed=1)

prices = jnp.ones(mmm.n_media_channels)

n_time_periods = 10
budget = jnp.sum(jnp.dot(prices, across_platform_media_data.mean(axis=0)))* n_time_periods

# Run optimization with the parameters of choice.
solution, kpi_without_optim, previous_media_allocation = optimize_media.find_optimal_budgets(
    n_time_periods=n_time_periods,
    media_mix_model=mmm,
    extra_features=extra_features_scaler.transform(extra_features_data_test)[:n_time_periods],
    budget=budget,
    prices=prices,
    media_scaler=media_scaler,
    target_scaler=target_scaler,
    seed=1)

# Obtain the optimal weekly allocation.
optimal_buget_allocation = prices * solution.x
optimal_buget_allocation

# similar renormalization to get previous budget allocation
previous_budget_allocation = prices * previous_media_allocation
previous_budget_allocation

# Both these values should be very close in order to compare KPI
budget, optimal_buget_allocation.sum()

# Both numbers should be almost equal
budget, jnp.sum(solution.x * prices)

# Plot out pre post optimization budget allocation and predicted target variable comparison.
plot.plot_pre_post_budget_allocation_comparison(media_mix_model=mmm,
                                                kpi_with_optim=solution['fun'],
                                                kpi_without_optim=kpi_without_optim,
                                                optimal_buget_allocation=optimal_buget_allocation,
                                                previous_budget_allocation=previous_budget_allocation,
                                                figure_size=(10,10))

mmm.print_summary()

plot.plot_media_channel_posteriors(media_mix_model=mmm)

plot.plot_prior_and_posterior(media_mix_model=mmm)

plot.plot_media_baseline_contribution_area_plot(media_mix_model=mmm,
                                                target_scaler=target_scaler,
                                                fig_size=(30,10))

adstock_models = ["adstock", "hill_adstock", "carryover"]
degrees_season = [1,2,3]

#adstock_models = ["hill_adstock"]
#degrees_season = [1]


for model_name in adstock_models:
    for degrees in degrees_season:
        mmm = lightweight_mmm.LightweightMMM(model_name=model_name)
        mmm.fit(media=media_data_train_scaled,
                media_prior=costs_scaled,
                target=target_train_scaled,
                extra_features=extra_features_scaled,
                number_warmup=1000,
                number_samples=1000,
                number_chains=1,

                seed=1)

        prediction = mmm.predict(
        media=media_data_test_scaled,
        extra_features=extra_features_test_scaled,
        target_scaler=target_scaler,
        seed=1)
        p = prediction.mean(axis=0)

        mape = mean_absolute_percentage_error(target_data_test, p)
        print(f"model_name={model_name} degrees={degrees} MAPE={mape} samples={p[:3]}")

adstock_models = ["adstock", "hill_adstock", "carryover"]
degrees_season = [1,2,3]

#adstock_models = ["hill_adstock"]
#degrees_season = [1]


for model_name in adstock_models:
    for degrees in degrees_season:
        mmm = lightweight_mmm.LightweightMMM(model_name=model_name)
        mmm.fit(media=media_data_train_scaled,
                media_prior=costs_scaled,
                target=target_train_scaled1,
                extra_features=extra_features_scaled,
                number_warmup=1000,
                number_samples=1000,
                number_chains=1,

                seed=1)

        prediction = mmm.predict(
        media=media_data_test_scaled,
        extra_features=extra_features_test_scaled,
        target_scaler=target_scaler,
        seed=1)
        p = prediction.mean(axis=0)

        mape = mean_absolute_percentage_error(target_data_test1, p)
        print(f"model_name={model_name} degrees={degrees} MAPE={mape} samples={p[:3]}")

len(across_platform_sales_data)

